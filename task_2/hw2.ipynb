{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Dense, Input, Masking, Lambda\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LambdaCallback, LearningRateScheduler, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_val(filename, val_prob=0.1):\n",
    "    with open(filename) as handler:\n",
    "        train_X, train_Y, val_X, val_Y = [], [], [], []\n",
    "        for line in handler:\n",
    "            x, y = line.strip().lower().split(\"\\t\")\n",
    "            x = re.sub(\"\\W\", \" \", x)\n",
    "            if np.random.uniform() > val_prob:\n",
    "                train_X.append(x)\n",
    "                train_Y.append(y)\n",
    "            else:\n",
    "                val_X.append(x)\n",
    "                val_Y.append(y)\n",
    "        return np.array(train_X), np.array(train_Y), np.array(val_X), np.array(val_Y)\n",
    "\n",
    "def read_test(filename):\n",
    "    with open(filename) as handler:\n",
    "        X = []\n",
    "        for line in handler:\n",
    "            line = re.sub(\"\\W\", \" \", line.strip().lower())\n",
    "            x = line.strip().lower()\n",
    "            X.append(re.sub(\"\\W\", \" \", x))\n",
    "        return np.array(X)\n",
    "\n",
    "def acc_metric(labels, predictions):\n",
    "    assert isinstance(labels, np.ndarray)\n",
    "    assert isinstance(predictions, np.ndarray)\n",
    "    return len(labels[labels == predictions]) / len(labels)\n",
    "\n",
    "def meashure_model(model, X, Y):\n",
    "    kf = KFold(n_splits=2)\n",
    "    metrics = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        train_X, test_X = X[train_index], X[test_index]\n",
    "        train_Y, test_Y = Y[train_index], Y[test_index]\n",
    "        model.fit(train_X, train_Y)\n",
    "        predictions = model.predict(test_X)\n",
    "        metrics.append(acc_metric(predictions, test_Y))\n",
    "    return np.mean(metrics), np.var(metrics)\n",
    "\n",
    "class MostPopularModel:\n",
    "    def fit(self, X, Y):\n",
    "        self._answer = Counter(Y).most_common()[0][0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._answer] * len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILENAME = \"names_and_rubrics_learn.tsv\"\n",
    "TEST_FILENAME = \"names_and_rubrics_test_no_rubric.tsv\"\n",
    "MIN_FREQ = 5\n",
    "\n",
    "TOKENS_NUM = 150767\n",
    "HIDDEN_DIM = 512\n",
    "MAX_LEN = 45\n",
    "LSTM_NUM = 2\n",
    "HIDDEN_LAYESR_NUM = 2\n",
    "BATCH_SIZE = 64\n",
    "LABELS_NUM = 1222\n",
    "ACTIVATION = 'tanh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, val_X, val_Y = read_train_val(TRAIN_FILENAME)\n",
    "test_X = read_test(TEST_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8026112, 8026112, 890990, 890990, 1000000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X), len(train_Y), len(val_X), len(val_Y), len(test_X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "most_popular_model = MostPopularModel()\n",
    "mean, var = meashure_model(most_popular_model, train_X, train_Y)\n",
    "print(mean, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokens_counter(*datas):\n",
    "    tokens_counter = Counter()\n",
    "    for lines in datas:\n",
    "        for line in lines:\n",
    "            for word in line.split():\n",
    "                tokens_counter[word] += 1\n",
    "    return tokens_counter\n",
    "\n",
    "def make_index_by_token(tokens_counter, min_freq):\n",
    "    tokens = [token for token in tokens_counter if tokens_counter[token] > min_freq]\n",
    "    return {\n",
    "        token: i\n",
    "        for i, token in enumerate(tokens)\n",
    "    }\n",
    "\n",
    "def make_label_by_y(*Ys):\n",
    "    all_Y = set()\n",
    "    for Y in Ys:\n",
    "        all_Y.update(Y)\n",
    "    return {\n",
    "        y: i\n",
    "        for i, y in enumerate(set(all_Y))\n",
    "    }\n",
    "\n",
    "def make_indices_by_label(Y, X, label_by_y, index_by_token):\n",
    "    indices_by_label = {\n",
    "        label_by_y[y]: []\n",
    "        for y in set(Y)\n",
    "    }\n",
    "    for x, y in zip(X, Y):\n",
    "        indices_by_label[label_by_y[y]].append(make_indices_from_x(x, index_by_token))\n",
    "    return indices_by_label\n",
    "\n",
    "def get_max_X_len(*Xs):\n",
    "    max_len = -1\n",
    "    for X in Xs:\n",
    "        max_len = max(max_len, max([len(x.split()) for x in X]))\n",
    "    return max_len\n",
    "\n",
    "def make_indices_from_x(x, index_by_token):\n",
    "    return [\n",
    "        index_by_token[token]\n",
    "        for token in x.split()\n",
    "        if token in index_by_token\n",
    "    ]\n",
    "\n",
    "def make_positives_data_gen(X, Y, label_by_y, index_by_token):\n",
    "    def positives_data_gen():\n",
    "        while True:\n",
    "            index = np.random.randint(0, len(X))\n",
    "            yield (\n",
    "                make_indices_from_x(X[index], index_by_token),\n",
    "                label_by_y[Y[index]]\n",
    "            )\n",
    "    return positives_data_gen()\n",
    "\n",
    "def make_nagative(label, labels_nd_array, indices_by_label):\n",
    "    negative_label = label\n",
    "    while negative_label == label:\n",
    "        negative_label = np.random.choice(labels_nd_array)\n",
    "    variants = indices_by_label[negative_label]\n",
    "    index = np.random.randint(0, len(variants))\n",
    "    return variants[index]\n",
    "\n",
    "def make_data_gen(X, Y, label_by_y, index_by_token, indices_by_label, half_batch_size):\n",
    "    labels_nd_array = np.array([label_by_y[y] for y in set(Y)])\n",
    "    positive_data_gen = make_positives_data_gen(X, Y, label_by_y, index_by_token)\n",
    "    tokens_eye = np.eye(len(index_by_token))\n",
    "    labels_eye = np.eye(len(label_by_y))\n",
    "    outputs = np.array([1] * half_batch_size + [-1] * half_batch_size)\n",
    "    def data_gen():\n",
    "        while True:\n",
    "            positive_labels = []\n",
    "            names_inputs = np.zeros([half_batch_size * 2, MAX_LEN])\n",
    "            for i in range(half_batch_size):\n",
    "                positive_tokens_indices, label = next(positive_data_gen)                \n",
    "                names_inputs[i, :len(positive_tokens_indices)] = positive_tokens_indices\n",
    "                positive_labels.append(label)\n",
    "            negative_labels = []\n",
    "            for i, label in enumerate(positive_labels):\n",
    "                negative_tokens_indices = make_nagative(label, labels_nd_array, indices_by_label)\n",
    "                names_inputs[half_batch_size + i, :len(negative_tokens_indices)] = negative_tokens_indices\n",
    "                negative_labels.append(label)\n",
    "                \n",
    "            yield (\n",
    "                {'name_input': names_inputs, 'label_input': labels_eye[positive_labels + negative_labels]},\n",
    "                {'output': outputs}\n",
    "            )\n",
    "    return data_gen()\n",
    "\n",
    "\n",
    "def make_classification_data_gen(X, Y, label_by_y, index_by_token, batch_size):\n",
    "    labels_nd_array = np.array([label_by_y[y] for y in set(Y)])\n",
    "    positive_data_gen = make_positives_data_gen(X, Y, label_by_y, index_by_token)\n",
    "    tokens_eye = np.eye(len(index_by_token))\n",
    "    labels_eye = np.eye(len(label_by_y))\n",
    "    def data_gen():\n",
    "        while True:\n",
    "            labels = []\n",
    "            inputs = np.zeros([batch_size, MAX_LEN])\n",
    "            for i in range(batch_size):\n",
    "                positive_tokens_indices, label = next(positive_data_gen)                \n",
    "                inputs[i, :len(positive_tokens_indices)] = positive_tokens_indices\n",
    "                labels.append(label)\n",
    "                \n",
    "            yield inputs, labels_eye[labels]\n",
    "    return data_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_by_y = make_label_by_y(train_Y, val_Y)\n",
    "tokens_counter = make_tokens_counter(train_X, val_X, test_X)\n",
    "index_by_token = make_index_by_token(tokens_counter, MIN_FREQ)\n",
    "indices_by_label_train = make_indices_by_label(train_Y, train_X, label_by_y, index_by_token)\n",
    "indices_by_label_val = make_indices_by_label(val_Y, val_X, label_by_y, index_by_token)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = make_data_gen(\n",
    "    train_X,\n",
    "    train_Y,\n",
    "    label_by_y,\n",
    "    index_by_token,\n",
    "    indices_by_label_train,\n",
    "    int(BATCH_SIZE / 2)\n",
    ")\n",
    "val_data = make_data_gen(\n",
    "    val_X,\n",
    "    val_Y,\n",
    "    label_by_y,\n",
    "    index_by_token,\n",
    "    indices_by_label_val,\n",
    "    int(BATCH_SIZE / 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cosine_proximity(y_true, y_pred):\n",
    "    return -K.mean(y_pred * y_true)\n",
    "\n",
    "def mean_positive_score(y_true, y_pred):\n",
    "    filter_mult = (y_true + 1) / 2\n",
    "    return K.mean(y_pred * filter_mult)\n",
    "\n",
    "def mean_positive_var(y_true, y_pred):\n",
    "    mean_positive = mean_positive_score(y_true, y_pred)\n",
    "    filter_mult = (y_true + 1) / 2\n",
    "    return K.mean((y_pred * filter_mult - mean_positive) ** 2)\n",
    "\n",
    "def get_pred(y_true, y_pred):\n",
    "    mean_positive = mean_positive_score(y_true, y_pred)\n",
    "    mean_negative = mean_negative_score(y_true, y_pred)\n",
    "    \n",
    "    threshold = (mean_positive + mean_negative) / 2\n",
    "    \n",
    "    positive_mult = (y_true + 1) / 2\n",
    "    negative_mult = (1 - y_true) / 2\n",
    "\n",
    "    return K.mean((y_pred * filter_mult - mean_positive) ** 2)\n",
    "\n",
    "def mean_negative_score(y_true, y_pred):\n",
    "    filter_mult = (1 - y_true) / 2\n",
    "    return K.mean(y_pred * filter_mult)\n",
    "\n",
    "def mean_negative_var(y_true, y_pred):\n",
    "    mean_negative = mean_negative_score(y_true, y_pred)\n",
    "    filter_mult = (1 - y_true) / 2\n",
    "    return K.mean((y_pred * filter_mult - mean_negative) ** 2)\n",
    "\n",
    "def normalize(embedding):\n",
    "    return K.l2_normalize(embedding, axis=-1)\n",
    "\n",
    "def dot_product(embeddings):\n",
    "    return K.sum(embeddings[0] * embeddings[1], axis=-1)\n",
    "\n",
    "def reshape_to_prediction(score):\n",
    "    return K.reshape(score, (-1, 1))\n",
    "\n",
    "def make_dssm_predictions(X, y_set, model, label_by_y, index_by_token, limit):\n",
    "    labels_eye = np.eye(len(y_set))\n",
    "    labels = labels_eye[[label_by_y[y] for y in y_set]]\n",
    "    predictions = []\n",
    "    for x in X[:limit]:\n",
    "        names_input = np.zeros(MAX_LEN)\n",
    "        indices = make_indices_from_x(x, index_by_token)\n",
    "        names_input[:len(indices)] = indices\n",
    "        names_input = names_input.reshape([1, MAX_LEN]).repeat(len(labels), axis=0)\n",
    "        scores = model.predict({\"name_input\": names_input, \"label_input\": labels}).reshape(-1)\n",
    "        predictions.append(np.argmax(scores))\n",
    "        print(scores[predictions[-1]])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "name_input = Input(shape=(MAX_LEN,), name=\"name_input\")  # shape: (BATCH_SIZE, MAX_LEN)\n",
    "masked = Masking(mask_value=0)(name_input)\n",
    "encoded = Embedding(TOKENS_NUM, HIDDEN_DIM)(masked)  # shape: (BATCH_SIZE, MAX_LEN, HIDDEN_DIM)\n",
    "for i in range(LSTM_NUM - 1):\n",
    "    encoded = Bidirectional(LSTM(HIDDEN_DIM, return_sequences=True))(encoded)  # shape: (BATCH_SIZE, HIDDEN_DIM)\n",
    "hidden = Bidirectional(LSTM(HIDDEN_DIM))(encoded)  # shape: (BATCH_SIZE, HIDDEN_DIM)\n",
    "for i in range(HIDDEN_LAYESR_NUM):\n",
    "    hidden = Dense(HIDDEN_DIM, activation=ACTIVATION)(hidden)  # shape: (BATCH_SIZE, HIDDEN_DIM)\n",
    "name_embedding =  Lambda(normalize)(hidden)  # shape: (BATCH_SIZE, HIDDEN_DIM)\n",
    "\n",
    "label_input = Input(shape=(LABELS_NUM,), name=\"label_input\")  # shape: (BATCH_SIZE, LABELS_NUM)\n",
    "hidden = Dense(HIDDEN_DIM, activation=ACTIVATION)(label_input)  # shape: (BATCH_SIZE, HIDDEN_DIM)\n",
    "for i in range(HIDDEN_LAYESR_NUM - 1):\n",
    "    hidden = Dense(HIDDEN_DIM, activation=ACTIVATION)(hidden)  # shape: (BATCH_SIZE, HIDDEN_DIM)\n",
    "label_embedding = Lambda(normalize)(hidden)  # shape: (BATCH_SIZE, HIDDEN_DIM)\n",
    "\n",
    "score = Lambda(dot_product)([label_embedding, name_embedding])\n",
    "prediction = Lambda(reshape_to_prediction, name=\"output\")(score)\n",
    "\n",
    "model = Model(inputs=[label_input, name_input], outputs=prediction)\n",
    "model.compile(\n",
    "    Adam(),\n",
    "    loss=my_cosine_proximity,\n",
    "    metrics=[mean_positive_score, mean_negative_score, mean_positive_var, mean_negative_var, 'acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCallback(Callback):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 models_folder,\n",
    "                 metrics_file,\n",
    "                 train_generator,\n",
    "                 test_generator,\n",
    "                 validation_steps,\n",
    "                 validation_batch_divider):\n",
    "        self._model = model\n",
    "        self._models_folder = models_folder\n",
    "        self._metrics_file = metrics_file\n",
    "        self._test_generator = test_generator\n",
    "        self._train_generator = train_generator\n",
    "        self._validation_steps = validation_steps\n",
    "        self._validation_batch_divider = validation_batch_divider\n",
    "        self._epoch = 0\n",
    "\n",
    "        self.history = {}\n",
    "        for name in self._model.metrics_names:\n",
    "            self.history[\"train_\" + name] = []\n",
    "            self.history[\"test_\" + name] = []\n",
    "            \n",
    "    def on_train_begin(self, logs):\n",
    "        if os.path.exists(self._models_folder):\n",
    "            shutil.rmtree(self._models_folder)\n",
    "            os.mkdir(self._models_folder)\n",
    "        if os.path.exists(self._metrics_file):\n",
    "            os.remove(self._metrics_file)\n",
    "            open(self._metrics_file, \"w\").close()\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        if batch % self._validation_batch_divider == 0:\n",
    "            test_evals = self._model.evaluate_generator(\n",
    "                self._test_generator,\n",
    "                steps=self._validation_steps\n",
    "            )\n",
    "            train_evals = self._model.evaluate_generator(\n",
    "                self._train_generator,\n",
    "                steps=self._validation_steps\n",
    "            )\n",
    "            for metric_name, metric in zip(self._model.metrics_names, train_evals):\n",
    "                self.history[\"train_\" + metric_name].append(metric)\n",
    "            for metric_name, metric in zip(self._model.metrics_names, test_evals):\n",
    "                self.history[\"test_\" + metric_name].append(metric)\n",
    "            short_model_name = \"epoch_{}_batch_{}\".format(\n",
    "                self._epoch,\n",
    "                batch)\n",
    "            metrics_string = short_model_name + \"_train_{}_test_{}\".format(\n",
    "                \"_\".join(map(str, train_evals)),\n",
    "                \"_\".join(map(str, test_evals))\n",
    "            )\n",
    "            with open(self._metrics_file, \"a\") as handler:\n",
    "                handler.write(metrics_string + \"\\n\")\n",
    "            \n",
    "            self._model.save_weights(os.path.join(self._models_folder, short_model_name))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self._epoch += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "evaluate_callback = EvaluateCallback(\n",
    "    model,\n",
    "    \"logs\",\n",
    "    \"metrics.txt\",\n",
    "    train_data,\n",
    "    val_data,\n",
    "    1000,\n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "history = model.fit_generator(\n",
    "    train_data,\n",
    "    steps_per_epoch=int(len(train_X) / (BATCH_SIZE * 2)),\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    callbacks=[evaluate_callback],\n",
    "    initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_train_data = make_classification_data_gen(\n",
    "    train_X,\n",
    "    train_Y,\n",
    "    label_by_y,\n",
    "    index_by_token,\n",
    "    BATCH_SIZE\n",
    ")\n",
    "classification_val_data = make_classification_data_gen(\n",
    "    val_X,\n",
    "    val_Y,\n",
    "    label_by_y,\n",
    "    index_by_token,\n",
    "    BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/icecream/code/venv3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "name_input = Input(shape=(MAX_LEN,))  # shape: (BATCH_SIZE, MAX_LEN)\n",
    "masked = Masking(mask_value=0)(name_input)\n",
    "encoded = Embedding(TOKENS_NUM, HIDDEN_DIM)(masked)  # shape: (BATCH_SIZE, MAX_LEN, HIDDEN_DIM)\n",
    "for i in range(LSTM_NUM - 1):\n",
    "    encoded = Bidirectional(LSTM(HIDDEN_DIM, return_sequences=True))(encoded)  # shape: (BATCH_SIZE, HIDDEN_DIM)\n",
    "hidden = Bidirectional(LSTM(HIDDEN_DIM))(encoded)  # shape: (BATCH_SIZE, HIDDEN_DIM)\n",
    "for i in range(HIDDEN_LAYESR_NUM):\n",
    "    hidden = Dense(HIDDEN_DIM, activation=ACTIVATION)(hidden)  # shape: (BATCH_SIZE, HIDDEN_DIM)\n",
    "\n",
    "for i in range(HIDDEN_LAYESR_NUM - 1):\n",
    "    hidden = Dense(HIDDEN_DIM, activation=ACTIVATION)(hidden)  # shape: (BATCH_SIZE, HIDDEN_DIM)\n",
    "scores = Dense(LABELS_NUM, activation='softmax')(hidden)  # shape: (BATCH_SIZE, LABELS_NUM)\n",
    "\n",
    "classificaton_model = Model(inputs=name_input, outputs=scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificaton_model.compile(\n",
    "    Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_callback_classification = EvaluateCallback(\n",
    "    classificaton_model,\n",
    "    \"logs_classification\",\n",
    "    \"metrics_classification.txt\",\n",
    "    classification_train_data,\n",
    "    classification_val_data,\n",
    "    1000,\n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/icecream/code/venv3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "     1/125408 [..............................] - ETA: 493:55:26 - loss: 7.1082 - acc: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/icecream/code/venv3/lib/python3.7/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1059.741733). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "     2/125408 [..............................] - ETA: 18761:12:55 - loss: 7.0974 - acc: 0.0078  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/icecream/code/venv3/lib/python3.7/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (529.871486). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    49/125408 [..............................] - ETA: 861:52:52 - loss: 6.3437 - acc: 0.0261"
     ]
    }
   ],
   "source": [
    "history = classificaton_model.fit_generator(\n",
    "    classification_train_data,\n",
    "    steps_per_epoch=int(len(train_X) / (BATCH_SIZE)),\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    callbacks=[evaluate_callback_classification],\n",
    "    initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
