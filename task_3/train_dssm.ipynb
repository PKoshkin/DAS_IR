{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urldefrag\n",
    "from urllib.request import urlopen\n",
    "from file_storage import FileStorage\n",
    "from urllib.parse import urljoin\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "\n",
    "from inscriptis import get_text\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Dense, Input, Masking, Lambda\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LambdaCallback, LearningRateScheduler, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_lens(filename):\n",
    "    max_query_len, max_document_len = -1, -1\n",
    "    with open(filename) as handler:\n",
    "        for line in handler:\n",
    "            query, document = line.split(\"\\t\")\n",
    "            query_len = len(query.split())\n",
    "            document_len = len(document.split())\n",
    "            if query_len > max_query_len:\n",
    "                max_query_len = query_len\n",
    "            if document_len > max_document_len:\n",
    "                max_document_len = document_len\n",
    "    return max_query_len, max_document_len\n",
    "\n",
    "def cycle_file(filename):\n",
    "    while True:\n",
    "        with open(filename) as f:\n",
    "            yield from f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_query_len, max_document_len = get_max_lens(TRAIN_DATA)\n",
    "max_document_len, max_query_len = (35840, 1475)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "DATA_SIZE = 500000\n",
    "QUERY_DICT_SIZE = 247074\n",
    "DOCUMENT_DICT_SIZE = 583954\n",
    "ACTIVATION = 'relu'\n",
    "HIDDEN_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seq_batches_generator(filename, batch_size, max_query_len, max_document_len):\n",
    "    with open(filename) as handler:\n",
    "        while True:\n",
    "            query_batch = np.zeros([batch_size, max_query_len])\n",
    "            docment_batch = np.zeros([batch_size, max_document_len])\n",
    "            for i in range(batch_size):\n",
    "                line = next(handler)\n",
    "                query, document = line.split(\"\\t\")\n",
    "                query = list(map(int, query.split()))\n",
    "                document = list(map(int, document.split()))\n",
    "                query_batch[i, :len(query)] = query\n",
    "                docment_batch[i, :len(document)] = document\n",
    "            yield query_batch, docment_batch\n",
    "\n",
    "def make_non_seq_batches_generator(filename, batch_size):\n",
    "    with open(filename) as handler:\n",
    "        while True:\n",
    "            query_batch = np.zeros([batch_size, QUERY_DICT_SIZE])\n",
    "            docment_batch = np.zeros([batch_size, DOCUMENT_DICT_SIZE])\n",
    "            for i in range(batch_size):\n",
    "                line = next(handler)\n",
    "                query, document = line.split(\"\\t\")\n",
    "                query = list(map(int, query.split()))\n",
    "                document = list(map(int, document.split()))\n",
    "                for word in query:\n",
    "                    query_batch[i, word] +=1\n",
    "                for word in document:\n",
    "                    docment_batch[i, word] +=1\n",
    "            yield query_batch, docment_batch\n",
    "\n",
    "def make_data_generator(positive_generator, negative_generator):\n",
    "    while True:\n",
    "        positive_query_batch, positive_docment_batch = next(positive_generator)\n",
    "        negative_query_batch, negative_docment_batch = next(negative_generator)\n",
    "        query_input = np.concatenate([positive_query_batch, negative_query_batch], axis=0)\n",
    "        document_input = np.concatenate([positive_docment_batch, negative_docment_batch], axis=0)\n",
    "        labels = np.concatenate(\n",
    "            [np.ones(len(positive_query_batch)), -1 * np.ones(len(negative_query_batch))\n",
    "        ]).reshape([-1, 1])\n",
    "        yield (\n",
    "            {'query_input': query_input, 'document_input': document_input},\n",
    "            {'output': labels}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cosine_proximity(y_true, y_pred):\n",
    "    return -K.mean(y_pred * y_true)\n",
    "\n",
    "def mean_positive_score(y_true, y_pred):\n",
    "    filter_mult = (y_true + 1) / 2\n",
    "    return K.mean(y_pred * filter_mult)\n",
    "\n",
    "def mean_positive_var(y_true, y_pred):\n",
    "    mean_positive = mean_positive_score(y_true, y_pred)\n",
    "    filter_mult = (y_true + 1) / 2\n",
    "    return K.mean((y_pred * filter_mult - mean_positive) ** 2)\n",
    "\n",
    "def get_pred(y_true, y_pred):\n",
    "    mean_positive = mean_positive_score(y_true, y_pred)\n",
    "    mean_negative = mean_negative_score(y_true, y_pred)\n",
    "    \n",
    "    threshold = (mean_positive + mean_negative) / 2\n",
    "    \n",
    "    positive_mult = (y_true + 1) / 2\n",
    "    negative_mult = (1 - y_true) / 2\n",
    "\n",
    "    return K.mean((y_pred * filter_mult - mean_positive) ** 2)\n",
    "\n",
    "def mean_negative_score(y_true, y_pred):\n",
    "    filter_mult = (1 - y_true) / 2\n",
    "    return K.mean(y_pred * filter_mult)\n",
    "\n",
    "def mean_negative_var(y_true, y_pred):\n",
    "    mean_negative = mean_negative_score(y_true, y_pred)\n",
    "    filter_mult = (1 - y_true) / 2\n",
    "    return K.mean((y_pred * filter_mult - mean_negative) ** 2)\n",
    "\n",
    "def normalize(embedding):\n",
    "    return K.l2_normalize(embedding, axis=-1)\n",
    "\n",
    "def dot_product(embeddings):\n",
    "    return K.sum(embeddings[0] * embeddings[1], axis=-1)\n",
    "\n",
    "def reshape_to_prediction(score):\n",
    "    return K.reshape(score, (-1, 1))\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    alpha = 1\n",
    "    return (\n",
    "        #my_cosine_proximity(y_true, y_pred) +\n",
    "        alpha * (1 + mean_negative_score(y_true, y_pred)) ** 2 +\n",
    "        alpha * (1 - mean_positive_score(y_true, y_pred)) ** 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_model(query_dict_size, document_dict_size, hidden_layers_num, activation, hidden_dim, lstm_num):\n",
    "    query_input = Input(shape=(max_query_len,), name=\"query_input\")  # shape: (BATCH_SIZE, max_query_len)\n",
    "    query_masked = Masking(mask_value=0)(query_input)\n",
    "    # shape: (BATCH_SIZE, max_query_len, hidden_dim)\n",
    "    query_encoded = Embedding(query_dict_size, hidden_dim)(query_masked)\n",
    "    for i in range(lstm_num - 1):\n",
    "        # shape: (BATCH_SIZE, hidden_dim)\n",
    "        query_encoded = Bidirectional(LSTM(hidden_dim, return_sequences=True))(query_encoded)\n",
    "    query_hidden = Bidirectional(LSTM(hidden_dim))(query_encoded)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    for i in range(hidden_layers_num):\n",
    "        query_hidden = Dense(hidden_dim, activation=activation)(query_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    query_embedding = Lambda(normalize)(query_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "\n",
    "    document_input = Input(shape=(max_document_len,), name=\"document_input\")  # shape: (BATCH_SIZE, max_document_len)\n",
    "    document_masked = Masking(mask_value=0)(document_input)\n",
    "    # shape: (BATCH_SIZE, max_document_len, hidden_dim)\n",
    "    document_encoded = Embedding(document_dict_size, hidden_dim)(document_masked)\n",
    "    for i in range(lstm_num - 1):\n",
    "        # shape: (BATCH_SIZE, hidden_dim)\n",
    "        document_encoded = Bidirectional(LSTM(hidden_dim, return_sequences=True))(document_encoded)\n",
    "    document_hidden = Bidirectional(LSTM(hidden_dim))(document_encoded)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    for i in range(hidden_layers_num):\n",
    "        document_hidden = Dense(hidden_dim, activation=activation)(document_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    label_embedding = Lambda(normalize)(document_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "\n",
    "    score = Lambda(dot_product)([label_embedding, query_embedding])\n",
    "    prediction = Lambda(reshape_to_prediction, name=\"output\")(score)\n",
    "\n",
    "    model = Model(inputs=[document_input, query_input], outputs=prediction)\n",
    "    model.compile(\n",
    "        Adam(),\n",
    "        loss=my_cosine_proximity,\n",
    "        metrics=[mean_positive_score, mean_negative_score, mean_positive_var, mean_negative_var, 'acc']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dense_model(query_dict_size, document_dict_size, hidden_layers_num, activation, hidden_dim):\n",
    "    query_input = Input(shape=(query_dict_size,), name=\"query_input\")  # shape: (BATCH_SIZE, QUERY_DICT_SIZE)\n",
    "    query_hidden = Dense(hidden_dim, activation=activation)(query_input)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    for i in range(hidden_layers_num - 1):\n",
    "        query_hidden = Dense(hidden_dim, activation=activation)(query_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    query_embedding = Lambda(normalize)(query_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "\n",
    "    # shape: (BATCH_SIZE, document_dict_size)\n",
    "    document_input = Input(shape=(document_dict_size,), name=\"document_input\")\n",
    "    document_hidden = Dense(hidden_dim, activation=activation)(document_input)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    for i in range(hidden_layers_num - 1):\n",
    "        document_hidden = Dense(hidden_dim, activation=activation)(document_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    label_embedding = Lambda(normalize)(document_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "\n",
    "    score = Lambda(dot_product)([label_embedding, query_embedding])\n",
    "    prediction = Lambda(reshape_to_prediction, name=\"output\")(score)\n",
    "\n",
    "    model = Model(inputs=[document_input, query_input], outputs=prediction)\n",
    "    model.compile(\n",
    "        Adam(),\n",
    "        loss=my_cosine_proximity,\n",
    "        metrics=[mean_positive_score, mean_negative_score, mean_positive_var, mean_negative_var, 'acc']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lstm_model = make_lstm_model(\n",
    "    QUERY_DICT_SIZE,\n",
    "    DOCUMENT_DICT_SIZE,\n",
    "    2,\n",
    "    ACTIVATION,\n",
    "    HIDDEN_DIM,\n",
    "    2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/icecream/code/venv3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "dense_model = make_dense_model(\n",
    "    QUERY_DICT_SIZE,\n",
    "    DOCUMENT_DICT_SIZE,\n",
    "    2,\n",
    "    ACTIVATION,\n",
    "    HIDDEN_DIM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/icecream/code/venv3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      " 196/7812 [..............................] - ETA: 16:12:17 - loss: -0.0140 - mean_positive_score: 0.1970 - mean_negative_score: 0.1829 - mean_positive_var: 0.0664 - mean_negative_var: 0.0590 - acc: 0.1555"
     ]
    }
   ],
   "source": [
    "history = dense_model.fit_generator(\n",
    "    make_data_generator(\n",
    "        make_non_seq_batches_generator(\"positive_train_data_35K.tsv\", BATCH_SIZE),\n",
    "        make_non_seq_batches_generator(\"negative_train_data_35K.tsv\", BATCH_SIZE)\n",
    "    ),\n",
    "    steps_per_epoch=int(DATA_SIZE / (BATCH_SIZE * 2)),\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
