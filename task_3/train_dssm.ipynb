{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Dense, Input, Masking, Lambda\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LambdaCallback, LearningRateScheduler, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_lens(filename):\n",
    "    max_query_len, max_document_len = -1, -1\n",
    "    with open(filename) as handler:\n",
    "        for line in handler:\n",
    "            query, document = line.split(\"\\t\")\n",
    "            query_len = len(query.split())\n",
    "            document_len = len(document.split())\n",
    "            if query_len > max_query_len:\n",
    "                max_qu\n",
    "                ery_len = query_len\n",
    "            if document_len > max_document_len:\n",
    "                max_document_len = document_len\n",
    "    return max_query_len, max_document_len\n",
    "\n",
    "def cycle_file(filename):\n",
    "    while True:\n",
    "        with open(filename) as f:\n",
    "            yield from f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_query_len, max_document_len = get_max_lens(TRAIN_DATA)\n",
    "max_document_len, max_query_len = (35840, 1475)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "DATA_SIZE = 500000\n",
    "QUERY_DICT_SIZE = 247074\n",
    "DOCUMENT_DICT_SIZE = 583954\n",
    "ACTIVATION = 'relu'\n",
    "HIDDEN_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seq_batches_generator(filename, batch_size, max_query_len, max_document_len):\n",
    "    with open(filename) as handler:\n",
    "        while True:\n",
    "            query_batch = np.zeros([batch_size, max_query_len])\n",
    "            docment_batch = np.zeros([batch_size, max_document_len])\n",
    "            for i in range(batch_size):\n",
    "                line = next(handler)\n",
    "                query, document = line.split(\"\\t\")\n",
    "                query = list(map(int, query.split()))\n",
    "                document = list(map(int, document.split()))\n",
    "                query_batch[i, :len(query)] = query\n",
    "                docment_batch[i, :len(document)] = document\n",
    "            yield query_batch, docment_batch\n",
    "\n",
    "def make_non_seq_batches_generator(filename, batch_size):\n",
    "    with open(filename) as handler:\n",
    "        while True:\n",
    "            query_batch = np.zeros([batch_size, QUERY_DICT_SIZE])\n",
    "            docment_batch = np.zeros([batch_size, DOCUMENT_DICT_SIZE])\n",
    "            for i in range(batch_size):\n",
    "                line = next(handler)\n",
    "                query, document = line.split(\"\\t\")\n",
    "                query = list(map(int, query.split()))\n",
    "                document = list(map(int, document.split()))\n",
    "                for word in query:\n",
    "                    query_batch[i, word] += 1\n",
    "                for word in document:\n",
    "                    docment_batch[i, word] += 1\n",
    "            yield query_batch, docment_batch\n",
    "\n",
    "def make_data_generator(positive_generator, negative_generator):\n",
    "    while True:\n",
    "        positive_query_batch, positive_docment_batch = next(positive_generator)\n",
    "        negative_query_batch, negative_docment_batch = next(negative_generator)\n",
    "        query_input = np.concatenate([positive_query_batch, negative_query_batch], axis=0)\n",
    "        document_input = np.concatenate([positive_docment_batch, negative_docment_batch], axis=0)\n",
    "        labels = np.concatenate(\n",
    "            [np.ones(len(positive_query_batch)), -1 * np.ones(len(negative_query_batch))\n",
    "        ]).reshape([-1, 1])\n",
    "        yield (\n",
    "            {'query_input': query_input, 'document_input': document_input},\n",
    "            {'output': labels}\n",
    "        )\n",
    "\n",
    "def make_batch(positive_lines_generator, negative_lines_generator, batch_size):\n",
    "    lines = [next(positive_lines_generator) for i in range(batch_size)]\n",
    "    lines += [next(negative_lines_generator) for i in range(batch_size)]\n",
    "    inputs = make_input_from_lines(lines)\n",
    "    labels = np.concatenate([np.ones(batch_size), -1 * np.ones(batch_size)]).reshape([-1, 1])\n",
    "    return inputs, {'output': labels}\n",
    "\n",
    "def make_input_from_lines(lines):\n",
    "    query_indices_axis_0, document_indices_axis_0 = [], []\n",
    "    query_indices_axis_1, document_indices_axis_1 = [], []\n",
    "    query_values, document_values = [], []\n",
    "    for i, line in enumerate(lines):\n",
    "        query, document = line.split(\"\\t\")\n",
    "        query_words = Counter(map(int, query.split()))\n",
    "        document_words = Counter(map(int, document.split()))\n",
    "\n",
    "        for word in query_words:\n",
    "            query_indices_axis_0.append(i)\n",
    "            query_indices_axis_1.append(word)\n",
    "            query_values.append(query_words[word])\n",
    "        for word in document_words:\n",
    "            document_indices_axis_0.append(i)\n",
    "            document_indices_axis_1.append(word)\n",
    "            document_values.append(document_words[word])\n",
    "\n",
    "    query_batch = csr_matrix(\n",
    "        (query_values, (query_indices_axis_0, query_indices_axis_1)),\n",
    "        shape=(len(lines), QUERY_DICT_SIZE)\n",
    "    )\n",
    "    docment_batch = csr_matrix(\n",
    "        (document_values, (document_indices_axis_0, document_indices_axis_1)),\n",
    "        shape=(len(lines), DOCUMENT_DICT_SIZE)\n",
    "    )\n",
    "\n",
    "    return {'query_input': query_batch, 'document_input': docment_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, positive_filename, negative_filename, model, batch_size, negative_candidates_num):\n",
    "        self._positive_generator = cycle_file(positive_filename)\n",
    "        self._negative_generator = cycle_file(negative_filename)\n",
    "        self._model = model\n",
    "        self._batch_size = batch_size\n",
    "        self._negative_candidates_num = negative_candidates_num\n",
    "\n",
    "    def make_batch(self):\n",
    "        lines = [next(self._negative_generator) for _ in range(self._negative_candidates_num)]\n",
    "        inputs = make_input_from_lines(lines)\n",
    "        predictions = -self._model.predict(inputs).reshape(-1)\n",
    "        indices = np.argsort(predictions)[:self._batch_size]\n",
    "        negative_lines = np.array(lines)[indices]\n",
    "        return make_batch(self._positive_generator, (line for line in negative_lines), self._batch_size)\n",
    "    \n",
    "def make_generator(data_generator):\n",
    "    while True:\n",
    "        yield data_generator.make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cosine_proximity(y_true, y_pred):\n",
    "    return -K.mean(y_pred * y_true)\n",
    "\n",
    "def mean_positive_score(y_true, y_pred):\n",
    "    filter_mult = (y_true + 1) / 2\n",
    "    return K.mean(y_pred * filter_mult)\n",
    "\n",
    "def mean_positive_var(y_true, y_pred):\n",
    "    mean_positive = mean_positive_score(y_true, y_pred)\n",
    "    filter_mult = (y_true + 1) / 2\n",
    "    return K.mean((y_pred * filter_mult - mean_positive) ** 2)\n",
    "\n",
    "def get_pred(y_true, y_pred):\n",
    "    mean_positive = mean_positive_score(y_true, y_pred)\n",
    "    mean_negative = mean_negative_score(y_true, y_pred)\n",
    "    \n",
    "    threshold = (mean_positive + mean_negative) / 2\n",
    "    \n",
    "    positive_mult = (y_true + 1) / 2\n",
    "    negative_mult = (1 - y_true) / 2\n",
    "\n",
    "    return K.mean((y_pred * filter_mult - mean_positive) ** 2)\n",
    "\n",
    "def mean_negative_score(y_true, y_pred):\n",
    "    filter_mult = (1 - y_true) / 2\n",
    "    return K.mean(y_pred * filter_mult)\n",
    "\n",
    "def mean_negative_var(y_true, y_pred):\n",
    "    mean_negative = mean_negative_score(y_true, y_pred)\n",
    "    filter_mult = (1 - y_true) / 2\n",
    "    return K.mean((y_pred * filter_mult - mean_negative) ** 2)\n",
    "\n",
    "def normalize(embedding):\n",
    "    return K.l2_normalize(embedding, axis=-1)\n",
    "\n",
    "def dot_product(embeddings):\n",
    "    return K.sum(embeddings[0] * embeddings[1], axis=-1)\n",
    "\n",
    "def reshape_to_prediction(score):\n",
    "    return K.reshape(score, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_branch(tensor, lstm_num, dense_num, hidden_size, dict_size, activation):\n",
    "    tensor = Masking(mask_value=0)(tensor)\n",
    "    tensor = Embedding(dict_size, hidden_size)(tensor)  # shape: (BATCH_SIZE, dict_size, hidden_size)\n",
    "    for i in range(lstm_num - 1):\n",
    "        tensor = Bidirectional(LSTM(hidden_size, return_sequences=True))(tensor)  # shape: (BATCH_SIZE, hidden_size)\n",
    "    tensor = Bidirectional(LSTM(hidden_size))(tensor)  # shape: (BATCH_SIZE, hidden_size)\n",
    "    for i in range(dense_num):\n",
    "        tensor = Dense(hidden_size, activation=activation)(tensor)  # shape: (BATCH_SIZE, hidden_size)\n",
    "    return Lambda(normalize)(tensor)  # shape: (BATCH_SIZE, hidden_size)\n",
    "\n",
    "def make_dense_branch(tensor, dense_num, hidden_size, dict_size, activation):\n",
    "    for i in range(dense_num):\n",
    "        tensor = Dense(hidden_size, activation=activation)(tensor)  # shape: (BATCH_SIZE, hidden_size)\n",
    "    return Lambda(normalize)(tensor)  # shape: (BATCH_SIZE, hidden_size)\n",
    "\n",
    "def make_compiled_model(input_1, input_2, embedding_1, embedding_2):\n",
    "    score = Lambda(dot_product)([embedding_1, embedding_2])\n",
    "    prediction = Lambda(reshape_to_prediction, name=\"output\")(score)\n",
    "\n",
    "    model = Model(inputs=[input_1, input_2], outputs=prediction)\n",
    "    model.compile(\n",
    "        Adam(),\n",
    "        loss=my_cosine_proximity,\n",
    "        metrics=[mean_positive_score, mean_negative_score, mean_positive_var, mean_negative_var, 'acc']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dense_model(document_dict_size,\n",
    "                     document_dense_num,\n",
    "                     query_dict_size,\n",
    "                     query_dense_num,\n",
    "                     activation,\n",
    "                     hidden_size):\n",
    "    # shape: (BATCH_SIZE, QUERY_DICT_SIZE)\n",
    "    query_input = Input(shape=(query_dict_size,), sparse=True, name=\"query_input\")\n",
    "    query_embedding = make_dense_branch(\n",
    "        query_input,\n",
    "        query_dense_num,\n",
    "        hidden_size,\n",
    "        query_dict_size,\n",
    "        activation\n",
    "    )\n",
    "    \n",
    "    # shape: (BATCH_SIZE, document_dict_size)\n",
    "    document_input = Input(shape=(document_dict_size,), sparse=True, name=\"document_input\")\n",
    "    document_embedding = make_dense_branch(\n",
    "        document_input,\n",
    "        document_dense_num,\n",
    "        hidden_size,\n",
    "        document_dict_size,\n",
    "        activation\n",
    "    )\n",
    "\n",
    "    return make_compiled_model(query_input, document_input, query_embedding, document_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCallback(Callback):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 models_folder,\n",
    "                 metrics_file,\n",
    "                 train_generator,\n",
    "                 test_generator,\n",
    "                 validation_steps,\n",
    "                 validation_batch_divider):\n",
    "        self._model = model\n",
    "        self._models_folder = models_folder\n",
    "        self._metrics_file = metrics_file\n",
    "        self._test_generator = test_generator\n",
    "        self._train_generator = train_generator\n",
    "        self._validation_steps = validation_steps\n",
    "        self._validation_batch_divider = validation_batch_divider\n",
    "        self._epoch = 0\n",
    "\n",
    "        self.history = {}\n",
    "        for name in self._model.metrics_names:\n",
    "            self.history[\"train_\" + name] = []\n",
    "            self.history[\"test_\" + name] = []\n",
    "            \n",
    "    def on_train_begin(self, logs):\n",
    "        if os.path.exists(self._models_folder):\n",
    "            shutil.rmtree(self._models_folder)\n",
    "        os.mkdir(self._models_folder)\n",
    "        if os.path.exists(self._metrics_file):\n",
    "            os.remove(self._metrics_file)\n",
    "        open(self._metrics_file, \"w\").close()\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        if batch % self._validation_batch_divider == 0:\n",
    "            test_evals = self._model.evaluate_generator(\n",
    "                self._test_generator,\n",
    "                steps=self._validation_steps\n",
    "            )\n",
    "            train_evals = self._model.evaluate_generator(\n",
    "                self._train_generator,\n",
    "                steps=self._validation_steps\n",
    "            )\n",
    "            for metric_name, metric in zip(self._model.metrics_names, train_evals):\n",
    "                self.history[\"train_\" + metric_name].append(metric)\n",
    "            for metric_name, metric in zip(self._model.metrics_names, test_evals):\n",
    "                self.history[\"test_\" + metric_name].append(metric)\n",
    "            short_model_name = \"epoch_{}_batch_{}\".format(\n",
    "                self._epoch,\n",
    "                batch)\n",
    "            metrics_string = short_model_name + \"_train_{}_test_{}\".format(\n",
    "                \"_\".join(map(str, train_evals)),\n",
    "                \"_\".join(map(str, test_evals))\n",
    "            )\n",
    "            with open(self._metrics_file, \"a\") as handler:\n",
    "                handler.write(metrics_string + \"\\n\")\n",
    "            \n",
    "            self._model.save_weights(os.path.join(self._models_folder, short_model_name))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self._epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dense_model = make_dense_model(\n",
    "    DOCUMENT_DICT_SIZE,\n",
    "    2,\n",
    "    QUERY_DICT_SIZE,\n",
    "    2,\n",
    "    ACTIVATION,\n",
    "    HIDDEN_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = make_generator(DataGenerator(\n",
    "    \"positive_train_data_35K.tsv\",\n",
    "    \"negative_train_data_35K.tsv\",\n",
    "    dense_model,\n",
    "    BATCH_SIZE,\n",
    "    BATCH_SIZE * 5\n",
    "))\n",
    "\n",
    "val_data_generator = make_generator(DataGenerator(\n",
    "    \"positive_train_data_35K.tsv\",\n",
    "    \"negative_train_data_35K.tsv\",\n",
    "    dense_model,\n",
    "    BATCH_SIZE,\n",
    "    BATCH_SIZE * 5\n",
    "))\n",
    "\n",
    "test_data_generator = make_generator(DataGenerator(\n",
    "    \"positive_train_data_35K.tsv\",\n",
    "    \"negative_train_data_35K.tsv\",\n",
    "    dense_model,\n",
    "    BATCH_SIZE,\n",
    "    BATCH_SIZE * 5\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "сallback = EvaluateCallback(\n",
    "     dense_model,\n",
    "     \"models\",\n",
    "     \"metrics\",\n",
    "     val_data_generator,\n",
    "     test_data_generator,\n",
    "     10,\n",
    "     5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(train_data_generator)\n",
    "y = next(val_data_generator)\n",
    "z = next(test_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   2/7812 [..............................] - ETA: 10:27:28 - loss: 0.0188 - mean_positive_score: 0.1511 - mean_negative_score: 0.1699 - mean_positive_var: 0.0237 - mean_negative_var: 0.0295 - acc: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icecream/.local/lib/python3.5/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (6.324775). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "/home/icecream/.local/lib/python3.5/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (3.163156). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6/7812 [..............................] - ETA: 3:46:26 - loss: 0.0102 - mean_positive_score: 0.1719 - mean_negative_score: 0.1821 - mean_positive_var: 0.0312 - mean_negative_var: 0.0347 - acc: 0.0078    "
     ]
    }
   ],
   "source": [
    "history = dense_model.fit_generator(\n",
    "    train_data_generator,\n",
    "    steps_per_epoch=int(DATA_SIZE / (BATCH_SIZE * 2)),\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    initial_epoch=0,\n",
    "    callbacks=[сallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
