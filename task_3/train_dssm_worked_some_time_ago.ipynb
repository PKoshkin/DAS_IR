{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Dense, Input, Masking, Lambda\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LambdaCallback, LearningRateScheduler, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_lens(filename):\n",
    "    max_query_len, max_document_len = -1, -1\n",
    "    with open(filename) as handler:\n",
    "        for line in handler:\n",
    "            query, document = line.split(\"\\t\")\n",
    "            query_len = len(query.split())\n",
    "            document_len = len(document.split())\n",
    "            if query_len > max_query_len:\n",
    "                max_qu\n",
    "                ery_len = query_len\n",
    "            if document_len > max_document_len:\n",
    "                max_document_len = document_len\n",
    "    return max_query_len, max_document_len\n",
    "\n",
    "def cycle_file(filename):\n",
    "    while True:\n",
    "        with open(filename) as f:\n",
    "            yield from f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_query_len, max_document_len = get_max_lens(TRAIN_DATA)\n",
    "max_document_len, max_query_len = (35840, 1475)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "DATA_SIZE = 500000\n",
    "QUERY_DICT_SIZE = 247074\n",
    "DOCUMENT_DICT_SIZE = 583954\n",
    "ACTIVATION = 'relu'\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seq_batches_generator(filename, batch_size, max_query_len, max_document_len):\n",
    "    with open(filename) as handler:\n",
    "        while True:\n",
    "            query_batch = np.zeros([batch_size, max_query_len])\n",
    "            docment_batch = np.zeros([batch_size, max_document_len])\n",
    "            for i in range(batch_size):\n",
    "                line = next(handler)\n",
    "                query, document = line.split(\"\\t\")\n",
    "                query = list(map(int, query.split()))\n",
    "                document = list(map(int, document.split()))\n",
    "                query_batch[i, :len(query)] = query\n",
    "                docment_batch[i, :len(document)] = document\n",
    "            yield query_batch, docment_batch\n",
    "\n",
    "def make_non_seq_batches_generator(filename, batch_size):\n",
    "    with open(filename) as handler:\n",
    "        while True:\n",
    "            query_batch = np.zeros([batch_size, QUERY_DICT_SIZE])\n",
    "            docment_batch = np.zeros([batch_size, DOCUMENT_DICT_SIZE])\n",
    "            for i in range(batch_size):\n",
    "                line = next(handler)\n",
    "                query, document = line.split(\"\\t\")\n",
    "                query = list(map(int, query.split()))\n",
    "                document = list(map(int, document.split()))\n",
    "                for word in query:\n",
    "                    query_batch[i, word] +=1\n",
    "                for word in document:\n",
    "                    docment_batch[i, word] +=1\n",
    "            yield query_batch, docment_batch\n",
    "\n",
    "def make_data_generator(positive_generator, negative_generator):\n",
    "    while True:\n",
    "        positive_query_batch, positive_docment_batch = next(positive_generator)\n",
    "        negative_query_batch, negative_docment_batch = next(negative_generator)\n",
    "        query_input = np.concatenate([positive_query_batch, negative_query_batch], axis=0)\n",
    "        document_input = np.concatenate([positive_docment_batch, negative_docment_batch], axis=0)\n",
    "        labels = np.concatenate(\n",
    "            [np.ones(len(positive_query_batch)), -1 * np.ones(len(negative_query_batch))\n",
    "        ]).reshape([-1, 1])\n",
    "        yield (\n",
    "            {'query_input': query_input, 'document_input': document_input},\n",
    "            {'output': labels}\n",
    "        )\n",
    "\n",
    "def make_sparse_data_generator(positive_filename, negative_filename, batch_size):\n",
    "    with open(positive_filename) as positive_handler, open(negative_filename) as negative_handler:\n",
    "        while True:\n",
    "            query_indices_axis_0, document_indices_axis_0 = [], []\n",
    "            query_indices_axis_1, document_indices_axis_1 = [], []\n",
    "            query_values, document_values = [], []\n",
    "            for handler in [positive_handler, negative_handler]:\n",
    "                for i in range(batch_size):\n",
    "                    line = next(handler)\n",
    "                    query, document = line.split(\"\\t\")\n",
    "                    query_words = Counter(map(int, query.split()))\n",
    "                    document_words = Counter(map(int, document.split()))\n",
    "\n",
    "                    for word in query_words:\n",
    "                        query_indices_axis_0.append(i)\n",
    "                        query_indices_axis_1.append(word)\n",
    "                        query_values.append(query_words[word])\n",
    "                    for word in document_words:\n",
    "                        document_indices_axis_0.append(i)\n",
    "                        document_indices_axis_1.append(word)\n",
    "                        document_values.append(document_words[word])\n",
    "                    \n",
    "            query_batch = csr_matrix(\n",
    "                (query_values, (query_indices_axis_0, query_indices_axis_1)),\n",
    "                shape=(BATCH_SIZE * 2, QUERY_DICT_SIZE)\n",
    "            )\n",
    "            docment_batch = csr_matrix(\n",
    "                (document_values, (document_indices_axis_0, document_indices_axis_1)),\n",
    "                shape=(BATCH_SIZE * 2, DOCUMENT_DICT_SIZE)\n",
    "            )\n",
    "            labels = np.concatenate([np.ones(batch_size), -1 * np.ones(batch_size)]).reshape([-1, 1])\n",
    "            yield (\n",
    "                {'query_input': query_batch, 'document_input': docment_batch},\n",
    "                {'output': labels}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_cosine_proximity(y_true, y_pred):\n",
    "    return -K.mean(y_pred * y_true)\n",
    "\n",
    "def mean_positive_score(y_true, y_pred):\n",
    "    filter_mult = (y_true + 1) / 2\n",
    "    return K.mean(y_pred * filter_mult)\n",
    "\n",
    "def mean_positive_var(y_true, y_pred):\n",
    "    mean_positive = mean_positive_score(y_true, y_pred)\n",
    "    filter_mult = (y_true + 1) / 2\n",
    "    return K.mean((y_pred * filter_mult - mean_positive) ** 2)\n",
    "\n",
    "def get_pred(y_true, y_pred):\n",
    "    mean_positive = mean_positive_score(y_true, y_pred)\n",
    "    mean_negative = mean_negative_score(y_true, y_pred)\n",
    "    \n",
    "    threshold = (mean_positive + mean_negative) / 2\n",
    "    \n",
    "    positive_mult = (y_true + 1) / 2\n",
    "    negative_mult = (1 - y_true) / 2\n",
    "\n",
    "    return K.mean((y_pred * filter_mult - mean_positive) ** 2)\n",
    "\n",
    "def mean_negative_score(y_true, y_pred):\n",
    "    filter_mult = (1 - y_true) / 2\n",
    "    return K.mean(y_pred * filter_mult)\n",
    "\n",
    "def mean_negative_var(y_true, y_pred):\n",
    "    mean_negative = mean_negative_score(y_true, y_pred)\n",
    "    filter_mult = (1 - y_true) / 2\n",
    "    return K.mean((y_pred * filter_mult - mean_negative) ** 2)\n",
    "\n",
    "def normalize(embedding):\n",
    "    return K.l2_normalize(embedding, axis=-1)\n",
    "\n",
    "def dot_product(embeddings):\n",
    "    return K.sum(embeddings[0] * embeddings[1], axis=-1)\n",
    "\n",
    "def reshape_to_prediction(score):\n",
    "    return K.reshape(score, (-1, 1))\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    alpha = 1\n",
    "    return (\n",
    "        #my_cosine_proximity(y_true, y_pred) +\n",
    "        alpha * (1 + mean_negative_score(y_true, y_pred)) ** 2 +\n",
    "        alpha * (1 - mean_positive_score(y_true, y_pred)) ** 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_model(query_dict_size, document_dict_size, hidden_layers_num, activation, hidden_dim, lstm_num):\n",
    "    query_input = Input(shape=(max_query_len,), sparse=True, name=\"query_input\")  # shape: (BATCH_SIZE, max_query_len)\n",
    "    query_masked = Masking(mask_value=0)(query_input)\n",
    "    # shape: (BATCH_SIZE, max_query_len, hidden_dim)\n",
    "    query_encoded = Embedding(query_dict_size, hidden_dim)(query_masked)\n",
    "    for i in range(lstm_num - 1):\n",
    "        # shape: (BATCH_SIZE, hidden_dim)\n",
    "        query_encoded = Bidirectional(LSTM(hidden_dim, return_sequences=True))(query_encoded)\n",
    "    query_hidden = Bidirectional(LSTM(hidden_dim))(query_encoded)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    for i in range(hidden_layers_num):\n",
    "        query_hidden = Dense(hidden_dim, activation=activation)(query_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    query_embedding = Lambda(normalize)(query_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "\n",
    "    # shape: (BATCH_SIZE, max_document_len)\n",
    "    document_input = Input(shape=(max_document_len,), sparse=True, name=\"document_input\")\n",
    "    document_masked = Masking(mask_value=0)(document_input)\n",
    "    # shape: (BATCH_SIZE, max_document_len, hidden_dim)\n",
    "    document_encoded = Embedding(document_dict_size, hidden_dim)(document_masked)\n",
    "    for i in range(lstm_num - 1):\n",
    "        # shape: (BATCH_SIZE, hidden_dim)\n",
    "        document_encoded = Bidirectional(LSTM(hidden_dim, return_sequences=True))(document_encoded)\n",
    "    document_hidden = Bidirectional(LSTM(hidden_dim))(document_encoded)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    for i in range(hidden_layers_num):\n",
    "        document_hidden = Dense(hidden_dim, activation=activation)(document_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    label_embedding = Lambda(normalize)(document_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "\n",
    "    score = Lambda(dot_product)([label_embedding, query_embedding])\n",
    "    prediction = Lambda(reshape_to_prediction, name=\"output\")(score)\n",
    "\n",
    "    model = Model(inputs=[document_input, query_input], outputs=prediction)\n",
    "    model.compile(\n",
    "        Adam(),\n",
    "        loss=my_cosine_proximity,\n",
    "        metrics=[mean_positive_score, mean_negative_score, mean_positive_var, mean_negative_var, 'acc']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dense_model(query_dict_size, document_dict_size, hidden_layers_num, activation, hidden_dim, sparse=False):\n",
    "    # shape: (BATCH_SIZE, QUERY_DICT_SIZE)\n",
    "    query_input = Input(shape=(query_dict_size,), sparse=sparse, name=\"query_input\")\n",
    "    query_hidden = Dense(hidden_dim, activation=activation)(query_input)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    for i in range(hidden_layers_num - 1):\n",
    "        query_hidden = Dense(hidden_dim, activation=activation)(query_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    query_embedding = Lambda(normalize)(query_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "\n",
    "    # shape: (BATCH_SIZE, document_dict_size)\n",
    "    document_input = Input(shape=(document_dict_size,), sparse=sparse, name=\"document_input\")\n",
    "    document_hidden = Dense(hidden_dim, activation=activation)(document_input)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    for i in range(hidden_layers_num - 1):\n",
    "        document_hidden = Dense(hidden_dim, activation=activation)(document_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "    label_embedding = Lambda(normalize)(document_hidden)  # shape: (BATCH_SIZE, hidden_dim)\n",
    "\n",
    "    score = Lambda(dot_product)([label_embedding, query_embedding])\n",
    "    prediction = Lambda(reshape_to_prediction, name=\"output\")(score)\n",
    "\n",
    "    model = Model(inputs=[document_input, query_input], outputs=prediction)\n",
    "    model.compile(\n",
    "        Adam(),\n",
    "        loss=my_cosine_proximity,\n",
    "        metrics=[mean_positive_score, mean_negative_score, mean_positive_var, mean_negative_var, 'acc']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lstm_model = make_lstm_model(\n",
    "    QUERY_DICT_SIZE,\n",
    "    DOCUMENT_DICT_SIZE,\n",
    "    2,\n",
    "    ACTIVATION,\n",
    "    HIDDEN_DIM,\n",
    "    2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model = make_dense_model(\n",
    "    QUERY_DICT_SIZE,\n",
    "    DOCUMENT_DICT_SIZE,\n",
    "    2,\n",
    "    ACTIVATION,\n",
    "    HIDDEN_DIM,\n",
    "    True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  41/7812 [..............................] - ETA: 40:01 - loss: -0.4400 - mean_positive_score: 0.4489 - mean_negative_score: 0.0088 - mean_positive_var: 0.2068 - mean_negative_var: 0.0021 - acc: 0.4855"
     ]
    }
   ],
   "source": [
    "history = dense_model.fit_generator(\n",
    "    make_sparse_data_generator(\"positive_train_data_35K.tsv\", \"negative_train_data_35K.tsv\", BATCH_SIZE),\n",
    "    steps_per_epoch=int(DATA_SIZE / (BATCH_SIZE * 2)),\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_gen = make_non_seq_batches_generator(\"positive_train_data_35K.tsv\", 1)\n",
    "negative_gen = make_non_seq_batches_generator(\"negative_train_data_35K.tsv\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_q, p_d = next(positive_gen)\n",
    "n_q, n_d = next(negative_gen)\n",
    "(\n",
    "    dense_model.predict({\"document_input\": p_d, \"query_input\": p_q})[0, 0],\n",
    "    dense_model.predict({\"document_input\": n_d, \"query_input\": n_q})[0, 0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_q, p_d = next(positive_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p_q.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = make_sparse_data_generator(\"positive_train_data_35K.tsv\", \"negative_train_data_35K.tsv\", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
