{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>rel</th>\n",
       "      <th>quid</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_38</th>\n",
       "      <th>feature_39</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.031310</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.033206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.646890</td>\n",
       "      <td>0.686107</td>\n",
       "      <td>0.823908</td>\n",
       "      <td>0.750092</td>\n",
       "      <td>0.385426</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.078682</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.080022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649824</td>\n",
       "      <td>0.578581</td>\n",
       "      <td>0.868557</td>\n",
       "      <td>0.641385</td>\n",
       "      <td>0.010462</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.074713</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.678161</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0.019058</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.022591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.918403</td>\n",
       "      <td>0.868457</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.863460</td>\n",
       "      <td>0.016642</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.040230</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.039477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.040555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565875</td>\n",
       "      <td>0.569440</td>\n",
       "      <td>0.769845</td>\n",
       "      <td>0.646567</td>\n",
       "      <td>0.073711</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.218391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   docid  rel  quid  feature_0  feature_1  feature_2  feature_3  feature_4  \\\n",
       "0      1    0    10   0.000000   0.000000       0.00   0.000000   0.000000   \n",
       "1      2    1    10   0.031310   0.666667       0.50   0.166667   0.033206   \n",
       "2      3    1    10   0.078682   0.166667       0.50   0.333333   0.080022   \n",
       "3      4    1    10   0.019058   1.000000       1.00   0.500000   0.022591   \n",
       "4      5    0    10   0.039477   0.000000       0.75   0.166667   0.040555   \n",
       "\n",
       "   feature_5  feature_6     ...      feature_36  feature_37  feature_38  \\\n",
       "0        0.0        0.0     ...        0.000000    0.000000    0.000000   \n",
       "1        0.0        0.0     ...        0.646890    0.686107    0.823908   \n",
       "2        0.0        0.0     ...        0.649824    0.578581    0.868557   \n",
       "3        0.0        0.0     ...        0.918403    0.868457    1.000000   \n",
       "4        0.0        0.0     ...        0.565875    0.569440    0.769845   \n",
       "\n",
       "   feature_39  feature_40  feature_41  feature_42  feature_43  feature_44  \\\n",
       "0    0.000000    0.000000    0.000000    0.017241    0.000000    0.000000   \n",
       "1    0.750092    0.385426    0.923077    0.086207    0.333333    0.448276   \n",
       "2    0.641385    0.010462    0.076923    0.074713    0.833333    0.678161   \n",
       "3    0.863460    0.016642    0.153846    0.040230    0.833333    0.896552   \n",
       "4    0.646567    0.073711    0.076923    0.034483    0.333333    0.218391   \n",
       "\n",
       "   feature_45  \n",
       "0         0.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         0.0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('train.tsv', sep='\\t').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batches(filename):\n",
    "    with open(filename, 'r') as handler:\n",
    "        next(handler)\n",
    "        x_by_q = defaultdict(lambda: [])\n",
    "        y_by_q = defaultdict(lambda: [])\n",
    "        for line in handler:\n",
    "            splited = list(map(float, line.strip().split()))\n",
    "            q = splited[2]\n",
    "            y_by_q[q].append(splited[1])\n",
    "            x_by_q[q].append(splited[3:])\n",
    "        X, Y = [], []\n",
    "        for q in x_by_q:\n",
    "            X.append(np.array(x_by_q[q]))\n",
    "            Y.append(np.array(y_by_q[q]))\n",
    "        return X, Y\n",
    "\n",
    "def train_test_split(X, Y, test_size=0.2):\n",
    "    assert len(X) == len(Y)\n",
    "    permutation = np.random.permutation(len(X))\n",
    "    split = int(len(permutation) * (1 - test_size))\n",
    "    train_indices = permutation[:split]\n",
    "    test_indices = permutation[split:]\n",
    "    return (\n",
    "        list(np.array(X)[train_indices]),\n",
    "        list(np.array(Y)[train_indices]),\n",
    "        list(np.array(X)[test_indices]),\n",
    "        list(np.array(Y)[test_indices]),\n",
    "    )\n",
    "\n",
    "def generate_data(X, Y):\n",
    "    permutation = np.random.permutation(len(X))\n",
    "    index = 0\n",
    "    while True:\n",
    "        yield X[permutation[index]], Y[permutation[index]]\n",
    "        index += 1\n",
    "        if index >= len(X):\n",
    "            index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Reference from https://gist.github.com/bwhite/3726239\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def dcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> dcg_at_k(r, 1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 1, method=1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 2)\n",
    "    5.0\n",
    "    >>> dcg_at_k(r, 2, method=1)\n",
    "    4.2618595071429155\n",
    "    >>> dcg_at_k(r, 10)\n",
    "    9.6051177391888114\n",
    "    >>> dcg_at_k(r, 11)\n",
    "    9.6051177391888114\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> ndcg_at_k(r, 1)\n",
    "    1.0\n",
    "    >>> r = [2, 1, 2, 0]\n",
    "    >>> ndcg_at_k(r, 4)\n",
    "    0.9203032077642922\n",
    "    >>> ndcg_at_k(r, 4, method=1)\n",
    "    0.96519546960144276\n",
    "    >>> ndcg_at_k([0], 1)\n",
    "    0.0\n",
    "    >>> ndcg_at_k([1], 2)\n",
    "    1.0\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n",
    "def get_ranked_relevances(model, X, Y):\n",
    "    all_ranked_relevances = []\n",
    "    for x, y in zip(X, Y):\n",
    "        prediction = model.predict(x)\n",
    "        assert len(prediction) == len(y)\n",
    "        pairs = list(zip(prediction, y))\n",
    "        ranked_relevances = [\n",
    "            y\n",
    "            for prediction, y in sorted(pairs, key=lambda pair: -pair[0])\n",
    "        ]\n",
    "        all_ranked_relevances.append(ranked_relevances)\n",
    "    return all_ranked_relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = read_batches('train.tsv')\n",
    "train_X, train_Y, test_X, test_Y = train_test_split(X, Y)\n",
    "features_num = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/icecream/code/venv3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def list_net_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        ytrue: tensor with shape [bs, 1]\n",
    "        y_pred: tensor with shape [bs, 1]\n",
    "    \"\"\"\n",
    "    y_true = K.squeeze(y_true, axis=1)  # shape: [bs]\n",
    "    y_pred = K.squeeze(y_pred, axis=1)  # shape: [bs]\n",
    "        \n",
    "    y_true_exp = K.exp(y_true)  # shape: [bs]\n",
    "    y_pred_exp = K.exp(2 * y_pred)  # shape: [bs]\n",
    "    \n",
    "    p_y = y_true_exp / K.sum(y_true_exp)  # shape: [bs]\n",
    "    p_f = y_pred_exp / K.sum(y_pred_exp)  # shape: [bs]\n",
    "    \n",
    "    return -K.sum(p_y * K.log(p_f))\n",
    "\n",
    "x = Input(shape=(features_num,))\n",
    "\n",
    "ranking_nn = Sequential()\n",
    "ranking_nn.add(Dense(64))\n",
    "ranking_nn.add(Activation('tanh'))\n",
    "ranking_nn.add(Dense(64))\n",
    "ranking_nn.add(Activation('tanh'))\n",
    "ranking_nn.add(Dense(64))\n",
    "ranking_nn.add(Activation('tanh'))\n",
    "ranking_nn.add(Dense(1))\n",
    "ranking_nn.add(Activation('tanh'))\n",
    "\n",
    "rank = ranking_nn(x)\n",
    "\n",
    "model = Model(inputs=x, outputs=rank)\n",
    "model.compile(loss=list_net_loss,\n",
    "              optimizer=Adam(lr=5e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21239309095448444\n"
     ]
    }
   ],
   "source": [
    "ranked_relevances = get_ranked_relevances(model, test_X, test_Y)\n",
    "ndcg = np.array([ndcg_at_k(r, k=5) for r in ranked_relevances]).mean()\n",
    "print(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.4644 - val_loss: 3.7000\n",
      "Epoch 2/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3855 - val_loss: 3.6776\n",
      "Epoch 3/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3748 - val_loss: 3.6714\n",
      "Epoch 4/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3702 - val_loss: 3.6680\n",
      "Epoch 5/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3674 - val_loss: 3.6659\n",
      "Epoch 6/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3655 - val_loss: 3.6644\n",
      "Epoch 7/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3642 - val_loss: 3.6633\n",
      "Epoch 8/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3632 - val_loss: 3.6625\n",
      "Epoch 9/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3625 - val_loss: 3.6619\n",
      "Epoch 10/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3619 - val_loss: 3.6614\n",
      "Epoch 11/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3614 - val_loss: 3.6610\n",
      "Epoch 12/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3610 - val_loss: 3.6607\n",
      "Epoch 13/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3607 - val_loss: 3.6604\n",
      "Epoch 14/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3604 - val_loss: 3.6601\n",
      "Epoch 15/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3602 - val_loss: 3.6599\n",
      "Epoch 16/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3600 - val_loss: 3.6597\n",
      "Epoch 17/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3598 - val_loss: 3.6595\n",
      "Epoch 18/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3596 - val_loss: 3.6593\n",
      "Epoch 19/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3594 - val_loss: 3.6592\n",
      "Epoch 20/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3593 - val_loss: 3.6591\n",
      "Epoch 21/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3591 - val_loss: 3.6589\n",
      "Epoch 22/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3590 - val_loss: 3.6588\n",
      "Epoch 23/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3589 - val_loss: 3.6587\n",
      "Epoch 24/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3587 - val_loss: 3.6586\n",
      "Epoch 25/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3586 - val_loss: 3.6585\n",
      "Epoch 26/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3585 - val_loss: 3.6584\n",
      "Epoch 27/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3584 - val_loss: 3.6583\n",
      "Epoch 28/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3584 - val_loss: 3.6583\n",
      "Epoch 29/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3583 - val_loss: 3.6582\n",
      "Epoch 30/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3582 - val_loss: 3.6581\n",
      "Epoch 31/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3581 - val_loss: 3.6580\n",
      "Epoch 32/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3580 - val_loss: 3.6580\n",
      "Epoch 33/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3580 - val_loss: 3.6579\n",
      "Epoch 34/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3579 - val_loss: 3.6579\n",
      "Epoch 35/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3578 - val_loss: 3.6578\n",
      "Epoch 36/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3578 - val_loss: 3.6578\n",
      "Epoch 37/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3577 - val_loss: 3.6577\n",
      "Epoch 38/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3577 - val_loss: 3.6577\n",
      "Epoch 39/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3576 - val_loss: 3.6577\n",
      "Epoch 40/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3576 - val_loss: 3.6576\n",
      "Epoch 41/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3575 - val_loss: 3.6576\n",
      "Epoch 42/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3575 - val_loss: 3.6575\n",
      "Epoch 43/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3574 - val_loss: 3.6575\n",
      "Epoch 44/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3574 - val_loss: 3.6575\n",
      "Epoch 45/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3574 - val_loss: 3.6574\n",
      "Epoch 46/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3573 - val_loss: 3.6574\n",
      "Epoch 47/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3573 - val_loss: 3.6574\n",
      "Epoch 48/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3572 - val_loss: 3.6574\n",
      "Epoch 49/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3572 - val_loss: 3.6573\n",
      "Epoch 50/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3572 - val_loss: 3.6573\n",
      "Epoch 51/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3571 - val_loss: 3.6573\n",
      "Epoch 52/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3571 - val_loss: 3.6573\n",
      "Epoch 53/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3571 - val_loss: 3.6572\n",
      "Epoch 54/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3570 - val_loss: 3.6572\n",
      "Epoch 55/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3570 - val_loss: 3.6572\n",
      "Epoch 56/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3570 - val_loss: 3.6572\n",
      "Epoch 57/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3570 - val_loss: 3.6572\n",
      "Epoch 58/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3569 - val_loss: 3.6571\n",
      "Epoch 59/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3569 - val_loss: 3.6571\n",
      "Epoch 60/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3569 - val_loss: 3.6571\n",
      "Epoch 61/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3568 - val_loss: 3.6571\n",
      "Epoch 62/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3568 - val_loss: 3.6571\n",
      "Epoch 63/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3568 - val_loss: 3.6571\n",
      "Epoch 64/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3568 - val_loss: 3.6570\n",
      "Epoch 65/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3567 - val_loss: 3.6570\n",
      "Epoch 66/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3567 - val_loss: 3.6570\n",
      "Epoch 67/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3567 - val_loss: 3.6570\n",
      "Epoch 68/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3567 - val_loss: 3.6570\n",
      "Epoch 69/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3566 - val_loss: 3.6570\n",
      "Epoch 70/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3566 - val_loss: 3.6569\n",
      "Epoch 71/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3566 - val_loss: 3.6569\n",
      "Epoch 72/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3566 - val_loss: 3.6569\n",
      "Epoch 73/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3565 - val_loss: 3.6569\n",
      "Epoch 74/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3565 - val_loss: 3.6569\n",
      "Epoch 75/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3565 - val_loss: 3.6569\n",
      "Epoch 76/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3565 - val_loss: 3.6569\n",
      "Epoch 77/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3564 - val_loss: 3.6569\n",
      "Epoch 78/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3564 - val_loss: 3.6568\n",
      "Epoch 79/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3564 - val_loss: 3.6568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3564 - val_loss: 3.6568\n",
      "Epoch 81/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3564 - val_loss: 3.6568\n",
      "Epoch 82/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3563 - val_loss: 3.6568\n",
      "Epoch 83/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3563 - val_loss: 3.6568\n",
      "Epoch 84/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3563 - val_loss: 3.6568\n",
      "Epoch 85/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3563 - val_loss: 3.6568\n",
      "Epoch 86/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3563 - val_loss: 3.6567\n",
      "Epoch 87/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3562 - val_loss: 3.6567\n",
      "Epoch 88/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3562 - val_loss: 3.6567\n",
      "Epoch 89/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3562 - val_loss: 3.6567\n",
      "Epoch 90/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3562 - val_loss: 3.6567\n",
      "Epoch 91/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3562 - val_loss: 3.6567\n",
      "Epoch 92/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3561 - val_loss: 3.6567\n",
      "Epoch 93/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3561 - val_loss: 3.6567\n",
      "Epoch 94/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3561 - val_loss: 3.6567\n",
      "Epoch 95/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3561 - val_loss: 3.6566\n",
      "Epoch 96/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3561 - val_loss: 3.6566\n",
      "Epoch 97/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3561 - val_loss: 3.6566\n",
      "Epoch 98/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3560 - val_loss: 3.6566\n",
      "Epoch 99/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3560 - val_loss: 3.6566\n",
      "Epoch 100/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3560 - val_loss: 3.6566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x104e3d358>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generate_data(train_X, train_Y),\n",
    "    epochs=100,\n",
    "    steps_per_epoch=len(train_X),\n",
    "    validation_data=generate_data(test_X, test_Y),\n",
    "    validation_steps=len(test_X),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4503403843902221\n",
      "0.4839925868421581\n"
     ]
    }
   ],
   "source": [
    "ranked_relevances = get_ranked_relevances(model, test_X, test_Y)\n",
    "ndcg = np.array([ndcg_at_k(r, k=5) for r in ranked_relevances]).mean()\n",
    "print(ndcg)\n",
    "ranked_relevances = get_ranked_relevances(model, test_X, test_Y)\n",
    "ndcg = np.array([ndcg_at_k(r, k=10) for r in ranked_relevances]).mean()\n",
    "print(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3560 - val_loss: 3.6565\n",
      "Epoch 2/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3560 - val_loss: 3.6565\n",
      "Epoch 3/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3560 - val_loss: 3.6564\n",
      "Epoch 4/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3560 - val_loss: 3.6564\n",
      "Epoch 5/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3560 - val_loss: 3.6564\n",
      "Epoch 6/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3559 - val_loss: 3.6564\n",
      "Epoch 7/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3559 - val_loss: 3.6564\n",
      "Epoch 8/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3559 - val_loss: 3.6564\n",
      "Epoch 9/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3559 - val_loss: 3.6564\n",
      "Epoch 10/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3559 - val_loss: 3.6564\n",
      "Epoch 11/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3559 - val_loss: 3.6564\n",
      "Epoch 12/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3558 - val_loss: 3.6564\n",
      "Epoch 13/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3558 - val_loss: 3.6564\n",
      "Epoch 14/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3558 - val_loss: 3.6564\n",
      "Epoch 15/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3558 - val_loss: 3.6563\n",
      "Epoch 16/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3558 - val_loss: 3.6563\n",
      "Epoch 17/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3558 - val_loss: 3.6563\n",
      "Epoch 18/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3558 - val_loss: 3.6563\n",
      "Epoch 19/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3557 - val_loss: 3.6563\n",
      "Epoch 20/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3557 - val_loss: 3.6563\n",
      "Epoch 21/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3557 - val_loss: 3.6563\n",
      "Epoch 22/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3557 - val_loss: 3.6563\n",
      "Epoch 23/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3557 - val_loss: 3.6563\n",
      "Epoch 24/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3557 - val_loss: 3.6563\n",
      "Epoch 25/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3557 - val_loss: 3.6563\n",
      "Epoch 26/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3556 - val_loss: 3.6563\n",
      "Epoch 27/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3556 - val_loss: 3.6563\n",
      "Epoch 28/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3556 - val_loss: 3.6563\n",
      "Epoch 29/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3556 - val_loss: 3.6562\n",
      "Epoch 30/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3556 - val_loss: 3.6562\n",
      "Epoch 31/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3556 - val_loss: 3.6562\n",
      "Epoch 32/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3556 - val_loss: 3.6562\n",
      "Epoch 33/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3556 - val_loss: 3.6562\n",
      "Epoch 34/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3555 - val_loss: 3.6562\n",
      "Epoch 35/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3555 - val_loss: 3.6562\n",
      "Epoch 36/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3555 - val_loss: 3.6562\n",
      "Epoch 37/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3555 - val_loss: 3.6562\n",
      "Epoch 38/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3555 - val_loss: 3.6562\n",
      "Epoch 39/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3555 - val_loss: 3.6562\n",
      "Epoch 40/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3555 - val_loss: 3.6562\n",
      "Epoch 41/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3555 - val_loss: 3.6562\n",
      "Epoch 42/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3554 - val_loss: 3.6562\n",
      "Epoch 43/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3554 - val_loss: 3.6562\n",
      "Epoch 44/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3554 - val_loss: 3.6562\n",
      "Epoch 45/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3554 - val_loss: 3.6562\n",
      "Epoch 46/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3554 - val_loss: 3.6562\n",
      "Epoch 47/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3554 - val_loss: 3.6562\n",
      "Epoch 48/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3554 - val_loss: 3.6561\n",
      "Epoch 49/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3554 - val_loss: 3.6561\n",
      "Epoch 50/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3553 - val_loss: 3.6561\n",
      "Epoch 51/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3553 - val_loss: 3.6561\n",
      "Epoch 52/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3553 - val_loss: 3.6561\n",
      "Epoch 53/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3553 - val_loss: 3.6561\n",
      "Epoch 54/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3553 - val_loss: 3.6561\n",
      "Epoch 55/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3553 - val_loss: 3.6561\n",
      "Epoch 56/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3553 - val_loss: 3.6561\n",
      "Epoch 57/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3553 - val_loss: 3.6561\n",
      "Epoch 58/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3553 - val_loss: 3.6561\n",
      "Epoch 59/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3552 - val_loss: 3.6561\n",
      "Epoch 60/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3552 - val_loss: 3.6561\n",
      "Epoch 61/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3552 - val_loss: 3.6561\n",
      "Epoch 62/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3552 - val_loss: 3.6561\n",
      "Epoch 63/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3552 - val_loss: 3.6561\n",
      "Epoch 64/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3552 - val_loss: 3.6561\n",
      "Epoch 65/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3552 - val_loss: 3.6561\n",
      "Epoch 66/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3552 - val_loss: 3.6561\n",
      "Epoch 67/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3552 - val_loss: 3.6561\n",
      "Epoch 68/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3551 - val_loss: 3.6561\n",
      "Epoch 69/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3551 - val_loss: 3.6561\n",
      "Epoch 70/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3551 - val_loss: 3.6561\n",
      "Epoch 71/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3551 - val_loss: 3.6561\n",
      "Epoch 72/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3551 - val_loss: 3.6561\n",
      "Epoch 73/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3551 - val_loss: 3.6561\n",
      "Epoch 74/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3551 - val_loss: 3.6560\n",
      "Epoch 75/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3551 - val_loss: 3.6560\n",
      "Epoch 76/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3551 - val_loss: 3.6560\n",
      "Epoch 77/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3551 - val_loss: 3.6560\n",
      "Epoch 78/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3550 - val_loss: 3.6560\n",
      "Epoch 79/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3550 - val_loss: 3.6560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3550 - val_loss: 3.6560\n",
      "Epoch 81/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3550 - val_loss: 3.6560\n",
      "Epoch 82/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3550 - val_loss: 3.6560\n",
      "Epoch 83/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3550 - val_loss: 3.6560\n",
      "Epoch 84/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3550 - val_loss: 3.6560\n",
      "Epoch 85/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3550 - val_loss: 3.6560\n",
      "Epoch 86/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3550 - val_loss: 3.6560\n",
      "Epoch 87/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3550 - val_loss: 3.6560\n",
      "Epoch 88/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3549 - val_loss: 3.6560\n",
      "Epoch 89/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3549 - val_loss: 3.6560\n",
      "Epoch 90/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3549 - val_loss: 3.6560\n",
      "Epoch 91/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3549 - val_loss: 3.6560\n",
      "Epoch 92/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3549 - val_loss: 3.6560\n",
      "Epoch 93/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3549 - val_loss: 3.6560\n",
      "Epoch 94/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3549 - val_loss: 3.6560\n",
      "Epoch 95/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3549 - val_loss: 3.6560\n",
      "Epoch 96/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3549 - val_loss: 3.6560\n",
      "Epoch 97/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3549 - val_loss: 3.6560\n",
      "Epoch 98/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3549 - val_loss: 3.6560\n",
      "Epoch 99/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3548 - val_loss: 3.6560\n",
      "Epoch 100/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3548 - val_loss: 3.6560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b850f98>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generate_data(train_X, train_Y),\n",
    "    epochs=100,\n",
    "    steps_per_epoch=len(train_X),\n",
    "    validation_data=generate_data(test_X, test_Y),\n",
    "    validation_steps=len(test_X),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45060973896374334\n",
      "0.4803527221402283\n"
     ]
    }
   ],
   "source": [
    "ranked_relevances = get_ranked_relevances(model, test_X, test_Y)\n",
    "ndcg = np.array([ndcg_at_k(r, k=5) for r in ranked_relevances]).mean()\n",
    "print(ndcg)\n",
    "ranked_relevances = get_ranked_relevances(model, test_X, test_Y)\n",
    "ndcg = np.array([ndcg_at_k(r, k=10) for r in ranked_relevances]).mean()\n",
    "print(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3548 - val_loss: 3.6559\n",
      "Epoch 2/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3548 - val_loss: 3.6559\n",
      "Epoch 3/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3548 - val_loss: 3.6559\n",
      "Epoch 4/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3548 - val_loss: 3.6559\n",
      "Epoch 5/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3548 - val_loss: 3.6559\n",
      "Epoch 6/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3548 - val_loss: 3.6559\n",
      "Epoch 7/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3548 - val_loss: 3.6559\n",
      "Epoch 8/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3548 - val_loss: 3.6559\n",
      "Epoch 9/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3548 - val_loss: 3.6559\n",
      "Epoch 10/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3547 - val_loss: 3.6559\n",
      "Epoch 11/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3547 - val_loss: 3.6559\n",
      "Epoch 12/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3547 - val_loss: 3.6559\n",
      "Epoch 13/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3547 - val_loss: 3.6559\n",
      "Epoch 14/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3547 - val_loss: 3.6559\n",
      "Epoch 15/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3547 - val_loss: 3.6559\n",
      "Epoch 16/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3547 - val_loss: 3.6559\n",
      "Epoch 17/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3547 - val_loss: 3.6559\n",
      "Epoch 18/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3547 - val_loss: 3.6559\n",
      "Epoch 19/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3547 - val_loss: 3.6559\n",
      "Epoch 20/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3547 - val_loss: 3.6559\n",
      "Epoch 21/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3547 - val_loss: 3.6559\n",
      "Epoch 22/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3546 - val_loss: 3.6559\n",
      "Epoch 23/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3546 - val_loss: 3.6559\n",
      "Epoch 24/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3546 - val_loss: 3.6559\n",
      "Epoch 25/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3546 - val_loss: 3.6559\n",
      "Epoch 26/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3546 - val_loss: 3.6559\n",
      "Epoch 27/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3546 - val_loss: 3.6559\n",
      "Epoch 28/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3546 - val_loss: 3.6559\n",
      "Epoch 29/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3546 - val_loss: 3.6559\n",
      "Epoch 30/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3546 - val_loss: 3.6559\n",
      "Epoch 31/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3546 - val_loss: 3.6559\n",
      "Epoch 32/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3546 - val_loss: 3.6559\n",
      "Epoch 33/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3546 - val_loss: 3.6559\n",
      "Epoch 34/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3545 - val_loss: 3.6559\n",
      "Epoch 35/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3545 - val_loss: 3.6559\n",
      "Epoch 36/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3545 - val_loss: 3.6559\n",
      "Epoch 37/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3545 - val_loss: 3.6559\n",
      "Epoch 38/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3545 - val_loss: 3.6559\n",
      "Epoch 39/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3545 - val_loss: 3.6559\n",
      "Epoch 40/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3545 - val_loss: 3.6559\n",
      "Epoch 41/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3545 - val_loss: 3.6559\n",
      "Epoch 42/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3545 - val_loss: 3.6559\n",
      "Epoch 43/100\n",
      "1190/1190 [==============================] - 2s 2ms/step - loss: 3.3545 - val_loss: 3.6559\n",
      "Epoch 44/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3545 - val_loss: 3.6559\n",
      "Epoch 45/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3545 - val_loss: 3.6559\n",
      "Epoch 46/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3544 - val_loss: 3.6559\n",
      "Epoch 47/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3544 - val_loss: 3.6559\n",
      "Epoch 48/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3544 - val_loss: 3.6559\n",
      "Epoch 49/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3544 - val_loss: 3.6559\n",
      "Epoch 50/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3544 - val_loss: 3.6558\n",
      "Epoch 51/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3544 - val_loss: 3.6558\n",
      "Epoch 52/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3544 - val_loss: 3.6558\n",
      "Epoch 53/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3544 - val_loss: 3.6558\n",
      "Epoch 54/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3544 - val_loss: 3.6558\n",
      "Epoch 55/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3544 - val_loss: 3.6558\n",
      "Epoch 56/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3544 - val_loss: 3.6558\n",
      "Epoch 57/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3544 - val_loss: 3.6558\n",
      "Epoch 58/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3544 - val_loss: 3.6558\n",
      "Epoch 59/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 60/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 61/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 62/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 63/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 64/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 65/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 66/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 67/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 68/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 69/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 70/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 71/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 72/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3543 - val_loss: 3.6558\n",
      "Epoch 73/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n",
      "Epoch 74/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n",
      "Epoch 75/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n",
      "Epoch 76/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n",
      "Epoch 77/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n",
      "Epoch 78/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n",
      "Epoch 79/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n",
      "Epoch 81/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n",
      "Epoch 82/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n",
      "Epoch 83/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n",
      "Epoch 84/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n",
      "Epoch 85/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n",
      "Epoch 86/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3542 - val_loss: 3.6558\n",
      "Epoch 87/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n",
      "Epoch 88/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n",
      "Epoch 89/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n",
      "Epoch 90/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n",
      "Epoch 91/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n",
      "Epoch 92/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n",
      "Epoch 93/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n",
      "Epoch 94/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n",
      "Epoch 95/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n",
      "Epoch 96/100\n",
      "1190/1190 [==============================] - 2s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n",
      "Epoch 97/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n",
      "Epoch 98/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n",
      "Epoch 99/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n",
      "Epoch 100/100\n",
      "1190/1190 [==============================] - 1s 1ms/step - loss: 3.3541 - val_loss: 3.6558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11e4a3198>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generate_data(train_X, train_Y),\n",
    "    epochs=100,\n",
    "    steps_per_epoch=len(train_X),\n",
    "    validation_data=generate_data(test_X, test_Y),\n",
    "    validation_steps=len(test_X),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45781311836107896\n",
      "0.48482830677696565\n"
     ]
    }
   ],
   "source": [
    "ranked_relevances = get_ranked_relevances(model, test_X, test_Y)\n",
    "ndcg = np.array([ndcg_at_k(r, k=5) for r in ranked_relevances]).mean()\n",
    "print(ndcg)\n",
    "ranked_relevances = get_ranked_relevances(model, test_X, test_Y)\n",
    "ndcg = np.array([ndcg_at_k(r, k=10) for r in ranked_relevances]).mean()\n",
    "print(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.tsv', sep='\\t')\n",
    "test.head()\n",
    "res = test[['docid', 'quid', 'feature_0']].copy()\n",
    "res['feature_0'] = model.predict(test[test.columns.delete((0, 1))])\n",
    "res.head()\n",
    "csv = res.sort_values(['quid', 'feature_0'], ascending=False)[['docid', 'quid']]\n",
    "csv.to_csv('rank_net_tanh.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
